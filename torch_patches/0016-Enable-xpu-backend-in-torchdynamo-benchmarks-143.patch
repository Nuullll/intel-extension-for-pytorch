From 372d4b8f53bba9ca207887dd36e9ba2d750ecbd0 Mon Sep 17 00:00:00 2001
From: "Wang, Chuanqi" <chuanqi.wang@intel.com>
Date: Mon, 24 Jul 2023 09:18:49 +0800
Subject: [PATCH 16/25] Enable xpu backend in torchdynamo benchmarks (#143)

---
 benchmarks/dynamo/common.py      | 26 ++++++++++++++++++++------
 benchmarks/dynamo/huggingface.py |  4 ++--
 benchmarks/dynamo/runner.py      |  9 +++++++++
 benchmarks/dynamo/timm_models.py |  4 ++--
 benchmarks/dynamo/torchbench.py  |  4 ++--
 5 files changed, 35 insertions(+), 12 deletions(-)

diff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py
index 1fbd012d823..c2e0bdc75ca 100644
--- a/benchmarks/dynamo/common.py
+++ b/benchmarks/dynamo/common.py
@@ -953,7 +953,7 @@ class BenchmarkRunner:
     def __init__(self):
         self.model_iter_fn = None
         self.grad_scaler = DummyGradScaler()
-        self.autocast = NullContext
+        self.autocast = NullContext()
         self.optimizer = None
         self._args = None
 
@@ -980,9 +980,17 @@ class BenchmarkRunner:
             #  factor between eager and dynamo run, making accuracy check
             #  harder.
             # self.grad_scaler = torch.cuda.amp.GradScaler(init_scale=2.0)
-            self.autocast = torch.cuda.amp.autocast
+            self.autocast = torch.cuda.amp.autocast()
+        elif self.args.amp and self.args.devices == ["xpu"]:
+            amp_dtype = os.getenv("INDUCTOR_AMP_DT")
+            if amp_dtype in ["fp16", "float16", "FP16", "FLOAT16"]:
+                amp_dtype = torch.float16
+            else:
+                amp_dtype = torch.bfloat16
+            print("Test amp with dt:", amp_dtype)
+            self.autocast = torch.xpu.amp.autocast(dtype=amp_dtype)
         elif self.args.bfloat16 and self.args.devices == ["cpu"]:
-            self.autocast = torch.cpu.amp.autocast
+            self.autocast = torch.cpu.amp.autocast()
 
     def init_optimizer(self, name, device, params):
         if device == "cuda" and self.args.training and name not in CI_SKIP_OPTIMIZER:
@@ -1464,7 +1472,7 @@ def parse_args(args=None):
         help="ID of the benchmark suite partition to be run. Used to divide CI tasks",
     )
     parser.add_argument(
-        "--devices", "--device", "-d", action="append", help="cpu or cuda"
+        "--devices", "--device", "-d", action="append", help="cpu, xpu or cuda"
     )
     parser.add_argument("--device-index", help="CUDA device index")
     parser.add_argument(
@@ -1939,13 +1947,19 @@ def run(runner, args, original_dir=None):
             log.warning("torch.cuda.is_available() == False, using CPU")
             args.devices = ["cpu"]
 
-    if args.devices != ["cpu"] and torch.cuda.is_available():
+    if "xpu" in args.devices:
+        import intel_extension_for_pytorch as ipex
+
+    if args.devices != ["cpu"] and (torch.cuda.is_available() or torch.xpu.is_available()):
         global synchronize
-        synchronize = torch.cuda.synchronize
+        synchronize = torch.cuda.synchronize if (torch.cuda.is_available()) else torch.xpu.synchronize
 
     if (
         args.devices == ["cuda"]
         and torch.cuda.get_device_properties(0).total_memory < 25 * 2**30
+    ) or (
+        args.devices == ["xpu"]
+        and torch.xpu.get_device_properties(0).total_memory < 25 * 2**30
     ):
         # OOM errors on an RTX 3090 with 24gb RAM
         runner.skip_models.update(
diff --git a/benchmarks/dynamo/huggingface.py b/benchmarks/dynamo/huggingface.py
index 893a50ccb94..8b2cc096e4d 100755
--- a/benchmarks/dynamo/huggingface.py
+++ b/benchmarks/dynamo/huggingface.py
@@ -481,13 +481,13 @@ class HuggingfaceRunner(BenchmarkRunner):
         return pred[0]
 
     def forward_pass(self, mod, inputs, collect_outputs=True):
-        with self.autocast():
+        with self.autocast:
             return mod(**inputs)
 
     def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
         cloned_inputs = clone_inputs(inputs)
         self.optimizer_zero_grad(mod)
-        with self.autocast():
+        with self.autocast:
             pred = mod(**cloned_inputs)
             loss = self.compute_loss(pred)
         self.grad_scaler.scale(loss).backward()
diff --git a/benchmarks/dynamo/runner.py b/benchmarks/dynamo/runner.py
index e21312ca15b..e879f9f4cfb 100755
--- a/benchmarks/dynamo/runner.py
+++ b/benchmarks/dynamo/runner.py
@@ -460,6 +460,15 @@ def build_summary(args):
         out_io.write(
             f"Device Memory [GB]: {torch.cuda.get_device_properties(0).total_memory/1e9}\n"
         )
+    
+    if "xpu" in args.devices:
+        out_io.write("\n")
+        out_io.write("### GPU details ###\n")
+        out_io.write(f"Number XPU Devices: {torch.xpu.device_count()}\n")
+        out_io.write(f"Device Name: {torch.xpu.get_device_name(0)}\n")
+        out_io.write(
+            f"Device Memory [GB]: {torch.xpu.get_device_properties(0).total_memory/1e9}\n"
+        )
 
     title = "## Build Summary"
     comment = generate_dropdown_comment(title, out_io.getvalue())
diff --git a/benchmarks/dynamo/timm_models.py b/benchmarks/dynamo/timm_models.py
index 905ea324c25..064415ae1d5 100755
--- a/benchmarks/dynamo/timm_models.py
+++ b/benchmarks/dynamo/timm_models.py
@@ -319,13 +319,13 @@ class TimmRunnner(BenchmarkRunner):
         return self.loss(pred, self.target) / 10.0
 
     def forward_pass(self, mod, inputs, collect_outputs=True):
-        with self.autocast():
+        with self.autocast:
             return mod(*inputs)
 
     def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
         cloned_inputs = clone_inputs(inputs)
         self.optimizer_zero_grad(mod)
-        with self.autocast():
+        with self.autocast:
             pred = mod(*cloned_inputs)
             if isinstance(pred, tuple):
                 pred = pred[0]
diff --git a/benchmarks/dynamo/torchbench.py b/benchmarks/dynamo/torchbench.py
index 48a7da1d2d5..90314234b66 100755
--- a/benchmarks/dynamo/torchbench.py
+++ b/benchmarks/dynamo/torchbench.py
@@ -358,13 +358,13 @@ class TorchBenchmarkRunner(BenchmarkRunner):
         return reduce_to_scalar_loss(pred)
 
     def forward_pass(self, mod, inputs, collect_outputs=True):
-        with self.autocast():
+        with self.autocast:
             return mod(*inputs)
 
     def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
         cloned_inputs = clone_inputs(inputs)
         self.optimizer_zero_grad(mod)
-        with self.autocast():
+        with self.autocast:
             pred = mod(*cloned_inputs)
             loss = self.compute_loss(pred)
         self.grad_scaler.scale(loss).backward()
-- 
2.34.1

